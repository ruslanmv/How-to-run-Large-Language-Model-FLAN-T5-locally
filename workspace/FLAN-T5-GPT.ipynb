{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e712df4a-4dcf-4d16-ad14-af02300fce17",
   "metadata": {},
   "source": [
    "# Large Language Model FLAN-T5 and GTP locally\n",
    "In this notebook we are going to run different versions of FLAN-T5 and GTP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8d001-fbe2-4394-9ce5-e5d33221ea9e",
   "metadata": {},
   "source": [
    "We define the following prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac09db50-5a1d-459a-abe9-8cd8b7469ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"A step by step recipe to make bolognese pasta:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd526f-bdf2-4f5b-9afd-9bae31e5d925",
   "metadata": {},
   "source": [
    "## FLAN-T5-small \n",
    "Here we are going to download 300 MB of data of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c14832-4231-4561-91fd-e5026732d1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusla\\.conda\\envs\\llm\\lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pour a cup of bolognese into a large bowl and add the pasta']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bfbc96-e044-44c5-91ee-9d931d918f30",
   "metadata": {},
   "source": [
    "## FLAN-T5-large\n",
    "Here we are going to download 3GB MB of data of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c456c886-8301-4054-8aa1-03f8c17671f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Toss the pasta with the sauce, then add the meat and toss again.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee33ec5-5a04-4461-bd96-f22530a84ac6",
   "metadata": {},
   "source": [
    "## CUDA Capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36beb749-5086-48af-a698-7bdae215d780",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This computer uses CUDA\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "is_cuda=torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    print(\"This computer uses CUDA\") \n",
    "else:  \n",
    "    print(\"This computer uses CPU\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8acc8e6-b37d-456c-ae78-11fcdcf9ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22501605-9dd5-4fe8-ba80-d6871cb95843",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5588be8f-52b1-47b7-9242-4e786b778ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Toss the pasta with the sauce, then add the meat and toss again.']\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfecb9c-fd22-40f2-b229-fc372e369cf5",
   "metadata": {},
   "source": [
    "##  GPT-neo-125M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dac3eda-b5f9-491d-95b4-a9e9845553d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b5d8a1-82a1-49c7-8fcd-88b2149a4c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "#outputs = model.generate(**input_ids)\n",
    "outputs = model.generate(**input_ids, do_sample=True, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "835fa15d-16c8-4ec6-b13d-50e8cde07344",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation =tokenizer.batch_decode(outputs, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d56ed07e-d8eb-4ddc-973d-ae5cc6cf8a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A step by step recipe to make bolognese pasta:\n",
      "1. Begin recipe steps #1-3. Prepare recipe ingredients carefully and use the pasta\n",
      "tubing tips (note: they should be in a bottle and not on the body of pasta). It is\n",
      "essential not to use the tubing tips for pasta in the recipe.\n",
      "3. Make topping.\n",
      "\n",
      "For pasta the tubing tips should be in a large bowl, leaving the pasta empty and\n",
      "sticking on as the pasta water is hot.\n",
      "4. Place the spaghetti and saucepan directly on a wire rack.\n",
      "5. To make sauce, spray with a thin coating. Using a fork, remove the pasta\n",
      "parting, then pour off any excess saucepan.\n",
      "6. Bring the pan down to just below the bottom of the spaghetti saucepan.\n",
      "7. Place the pans in the skillet and sprinkle the meat filling and sauce on top. As you\n",
      "add meat, pour the sauce from the bottom of the saucepan onto the bottom of\n",
      "the saucepan. Cover the pan evenly with towels. Let the sauce cook while you\n",
      "cooking.\n",
      "8. Put the pan in the oven, and cook for about 10 minutes. Remove from the\n",
      "pan and allow the meat to come to room temperature, about 15 minutes.\n",
      "9. Place the pan into the oven and cook for at least another 1 hour or so until the\n",
      "meat has used its space.\n",
      "10. Remove the spaghetti from the saucepan and place in the refrigerator\n",
      "while the sauce remains at room temperature.\n",
      "11. Now the pan is covered in paper towels.\n",
      "12. Place the spaghetti in the pans, cover the pans with plastic wrap and\n",
      "pour the sauce and saucepan into the bottom of this pan.\n",
      "\n",
      "When the sauce has cooled to a room temperature, place a knife or fork\n",
      "inside the pasta tub.\n",
      "If you get a really deep bowl in which the pasta is\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(generation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96444f-1a47-4d8a-bdd6-598c40ff9258",
   "metadata": {},
   "source": [
    "# HuggingFace Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecc84faf-5e5b-418a-aa83-8151bea75672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusla\\AppData\\Local\\Temp\\ipykernel_9536\\2730862050.py:26: GradioDeprecationWarning: `enable_queue` is deprecated in `Interface()`, please use it within `launch()` instead.\n",
      "  iface = gr.Interface(fn=app.begin,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import gradio as gr\n",
    "import re\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "class GUI:\n",
    "    def query(self,query,tokens=30):\n",
    "        options=\"\"\n",
    "        tok_len=tokens\n",
    "        t5query = f\"\"\"Question: \"{query}\" Context: {options}\"\"\"\n",
    "        inputs = tokenizer(t5query, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=tok_len)\n",
    "        return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "    def begin(self,question,tokens):\n",
    "        results = app.query(question,tokens)\n",
    "        return results\n",
    "\n",
    "app = GUI()\n",
    "title = \"Get answers with questions with Flan-T5\"\n",
    "description = \"Results will show up in a few seconds.\"\n",
    "css = \"\"\".output_image, .input_image {height: 600px !important}\"\"\"\n",
    "\n",
    "iface = gr.Interface(fn=app.begin, \n",
    "                     inputs=[ gr.Textbox(label=\"Question\"),\n",
    "                             gr.Slider(30, 100, value=30, step = 1)\n",
    "                            ],\n",
    "                     outputs = gr.Text(label=\"Answer Summary\"),\n",
    "                     title=title,\n",
    "                     description=description,\n",
    "                     #article=article,\n",
    "                     css=css,\n",
    "                     analytics_enabled = True, enable_queue=True)\n",
    "iface.launch(inline=False, share=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a51e747b-028c-48e0-b32d-12e51d293bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c78f3a-1666-4bbd-bb74-dc4254544cbb",
   "metadata": {},
   "source": [
    "## FLAN-T5 vs GTP Neo\n",
    "Now let us merge all tree models together and check the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "733116d3-7a26-40a4-872c-fa94011dc19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusla\\AppData\\Local\\Temp\\ipykernel_9536\\248601103.py:34: GradioDeprecationWarning: `enable_queue` is deprecated in `Interface()`, please use it within `launch()` instead.\n",
      "  iface = gr.Interface(fn=app.begin,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import torch\n",
    "import gradio as gr\n",
    "import re\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class GUI:\n",
    "    def query(self,query,modelo=\"flan-t5-small\",tokens=100):\n",
    "        options=\"\"\n",
    "        tok_len=tokens\n",
    "        t5query = f\"\"\"Question: \"{query}\" Context: {options}\"\"\"        \n",
    "        if (modelo==\"flan-t5-small\" or modelo==\"flan-t5-large\"):\n",
    "           tokenizer = AutoTokenizer.from_pretrained(\"google/{}\".format(modelo))\n",
    "           model = AutoModelForSeq2SeqLM.from_pretrained(\"google/{}\".format(modelo)).to(device)\n",
    "           inputs = tokenizer(t5query, return_tensors=\"pt\").to(device)\n",
    "           outputs = model.generate(**inputs, max_new_tokens=tok_len)\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\").to(device)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "            input_ids = tokenizer(t5query, return_tensors=\"pt\").to(device) \n",
    "            outputs = model.generate(**input_ids, do_sample=True, max_length=tok_len) \n",
    "        generation=tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        return '\\n'.join(generation)\n",
    "    def begin(self,question,modelo,tokens):\n",
    "        results = app.query(question,tokens)\n",
    "        return results\n",
    "\n",
    "app = GUI()\n",
    "title = \"Get answers with questions with Flan-T5\"\n",
    "description = \"Results will show up in a few seconds.\"\n",
    "article=\"More info  <a href='https://ruslanmv.com/'>ruslanmv.com</a><br>\" \n",
    "css = \"\"\".output_image, .input_image {height: 600px !important}\"\"\"\n",
    "\n",
    "iface = gr.Interface(fn=app.begin, \n",
    "                     inputs=[ gr.Textbox(label=\"Question\"),\n",
    "                     gr.Radio([\"flan-t5-small\", \"flan-t5-large\",\"gpt-neo-125M\"],label=\"Model\",value=\"flan-t5-small\"),\n",
    "                     gr.Slider(30, 200, value=100, step = 1,label=\"Max Tokens\"),],\n",
    "                     outputs = gr.Text(label=\"Answer Summary\"),\n",
    "                     title=title,\n",
    "                     description=description,\n",
    "                     article=article,\n",
    "                     css=css,\n",
    "                     analytics_enabled = True\n",
    "                    ,enable_queue=True)\n",
    "iface.launch(inline=False, share=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1589c3c4-52e3-4d68-8aff-3948280ab612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1625e8-e8da-4096-a75d-4e81fda5a6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
